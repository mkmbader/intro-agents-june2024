{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An advanced look at LLM Agents\n",
    "\n",
    "Previously, we relied on LangChain to to build the agent. However, this is not necessary, since an agent is nothing more than a fancy while loop. \n",
    "\n",
    "**The goal** \n",
    "\n",
    "With this notebook you will see what an agent is under the hood, and you can build it based on the LLM output\n",
    "\n",
    "ðŸŒŸ So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies\n",
    "\n",
    "Compile the cell below to install the dependencies. Consider clearing the cell output so it does not clutter your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv venv         \n",
    "!source venv/bin/activate     \n",
    "!pip3 install -r helper_functions/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The key elements\n",
    "\n",
    "The key elements of the agent remain - we need tools, an LLM that functions as the agent and a prompt with the query. In our example, we'll use ``chatgpt35-turbo`` as the agent LLM, [here](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) you find the documentation on how to query the model through the OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tools\n",
    "\n",
    "**Exercise:** Compile the cell below and check out the description. How does it differ from the previous tool description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.helper_functions import simple_description_formatter\n",
    "from helper_functions.tools import my_own_wiki_tool, weather_tool\n",
    "import pprint\n",
    "\n",
    "# load the tools and format. In plain OpenAI jargon they are called functions.\n",
    "tools = [my_own_wiki_tool, weather_tool]\n",
    "function_description = [simple_description_formatter(tool) for tool in tools]\n",
    "pprint.pprint(function_description[0])\n",
    "\n",
    "# Store executable functions with their name in dictionary\n",
    "available_functions = {tool.name: tool for tool in tools}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LLM output\n",
    "\n",
    "the prompt is feed as part of the messages object. All input, such as the system prompt, the user question, possibly also chat-history is provided through this object.\n",
    "\n",
    "**Exercise:** compile both questions, and compare the answers. Do you see a difference?\n",
    "\n",
    "Hint: The LLM can output two types of answers:\n",
    "* a string that answers the question, \n",
    "* a function-call object, which contains information on which function to call with which arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from helper_functions.keys import client\n",
    "\n",
    "\n",
    "system_prompt = \"You are a friendly, helpful assistant. Your goal is to answer the questions in a concise, but conversational manner.\"\n",
    "\n",
    "questions = [\"what is the meaning of life?\",\"How many people live in Paris?\"]\n",
    "\n",
    "for question in questions:\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "  \n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools = function_description,\n",
    "    messages=messages, \n",
    "    )\n",
    "\n",
    "  print(f\"Question: {question}\")\n",
    "  print(f\"Answer: {response.choices[0].message.content}\")\n",
    "  print(f\"Fuction call: {response.choices[0].message.tool_calls}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agent - a fancy while loop\n",
    "\n",
    "While the LLM requests function calls we \n",
    "* per function, **extract** the **name and arguments** to be called from the initial LLM response,\n",
    "* **execute** the **function calls**,\n",
    "* **store** the **output of the function** in the messages object,\n",
    "* invoke the LLM again, until no function call are requested.\n",
    "\n",
    "For more details, you can also check out this [OpenAI function calling guide](https://platform.openai.com/docs/guides/function-calling).\n",
    "\n",
    "**Exercise:** Change the question and investigate the output. Do you understand what you see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Initial LLM response ====\n",
      "Answer: None\n",
      "Function call: [ChatCompletionMessageToolCall(id='call_QiDKtvRh1w29vxbwATS2TECo', function=Function(arguments='{\"query\":\"King of the Netherlands\"}', name='wikipedia'), type='function')]\n",
      "\n",
      "==== Function call ====\n",
      "Calling function \"wikipedia\" with arguments {'query': 'King of the Netherlands'}.\n",
      "Function call response:\n",
      "Page: Monarchy of the Netherlands\n",
      "Summary: The monarchy of the Netherlands is a constitutional monarchy whose role and position are governed by the Constitution of the Netherlands. Roughly a third of the Constitution explains the succession, mechanisms of accession and abdication to the throne, the roles and duties of the monarch, the formalities of communication between the States General of the Netherlands, and the monarch's role in creating laws.\n",
      "The Kingdom of the Netherlands has been an independent monarchy since 16 March 1815. Its once-sovereign provinces had been intermittently ruled by members of the House of Orange-Nassau and the House of Nassau from 1559, when Philip II of Spain appointed William of Orange as stadtholder, until 1795, when the last stadtholder fled the country. \n",
      "William of Orange became the leader of the Dutch Revolt and of the independent Dutch Republic. Some of his descendants were later appointed as stateholders by some of the provinces. In 1747, the function of stateholder became a hereditary position in all provinces of the thus \"crowned\" Dutch Republic. The last stadtholder was William V, Prince of Orange.\n",
      "Willem-Alexander has been King of the Netherlands since 30 April 2013.\n",
      "\n",
      "== Intermediate LLM response ==\n",
      "Answer: The current King of the Netherlands is Willem-Alexander, who has been in reign since April 30, 2013. The monarchy of the Netherlands is a constitutional monarchy governed by the Constitution of the Netherlands. The role and position of the monarch are defined by the constitution, including succession, accession, and the monarch's duties.\n",
      "Function call: None\n",
      "\n",
      "==== Final LLM response ====\n",
      "Question:  Tell me some juicy details of the king of the netherlands\n",
      "Answer: The current King of the Netherlands is Willem-Alexander, who has been in reign since April 30, 2013. The monarchy of the Netherlands is a constitutional monarchy governed by the Constitution of the Netherlands. The role and position of the monarch are defined by the constitution, including succession, accession, and the monarch's duties.\n",
      "Function call: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# question = \"How many people live in Paris?\"\n",
    "# question = \"which city is bigger: Paris or Munich?\"\n",
    "question = 'Tell me some juicy details of the king of the netherlands'\n",
    "\n",
    "messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools = function_description,\n",
    "  messages=messages, \n",
    "  )\n",
    "\n",
    "print('==== Initial LLM response ====')\n",
    "print(f\"Answer: {response.choices[0].message.content}\")\n",
    "print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")\n",
    "\n",
    "# while the response requests function calls\n",
    "while response.choices[0].message.tool_calls:\n",
    "    \n",
    "  # store response message with all function calls\n",
    "  response_message = response.choices[0].message\n",
    "  messages.append(response_message)\n",
    "\n",
    "  # execute each tool individually\n",
    "  for tool_call in response.choices[0].message.tool_calls:\n",
    "    print('==== Function call ====')\n",
    "\n",
    "    # function name and arguments\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    print(f'Calling function \"{function_name}\" with arguments {function_args}.')\n",
    "\n",
    "    # execute function call \n",
    "    function_response = available_functions[function_name].invoke(function_args)\n",
    "    print(f'Function call response:\\n{function_response}\\n')\n",
    "\n",
    "    # append function response to messages\n",
    "    messages.append({\n",
    "        \"tool_call_id\":tool_call.id, \n",
    "        \"role\": \"tool\", \n",
    "        \"name\": function_name, \n",
    "        \"content\": function_response\n",
    "    })\n",
    "    \n",
    "  # get a new response from LLM\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools = function_description,\n",
    "    messages=messages, \n",
    "  )\n",
    "\n",
    "  print('== Intermediate LLM response ==')\n",
    "  print(f\"Answer: {response.choices[0].message.content}\")\n",
    "  print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")\n",
    "\n",
    "print('==== Final LLM response ====')\n",
    "print(\"Question: \", question)\n",
    "print(f\"Answer: {response.choices[0].message.content}\")\n",
    "print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
